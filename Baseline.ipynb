{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d613815",
   "metadata": {},
   "source": [
    "# **Sexual Predator Identification Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28e5f0",
   "metadata": {},
   "source": [
    "### 0. 1. Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740516b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afd5bb",
   "metadata": {},
   "source": [
    "### 0.2. Load data. Predator's ID and Chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f65600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pan12-sexual-predator-identification-training-corpus-predators-2012-05-01.txt\") as f:\n",
    "    predator_ids_train = set(f.read().splitlines())\n",
    "\n",
    "tree_train = ET.parse(\"pan12-sexual-predator-identification-training-corpus-2012-05-01.xml\")\n",
    "root_train = tree_train.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c7611",
   "metadata": {},
   "source": [
    "### 1. **Filtering stage.**\n",
    "\n",
    "Conversations with only one participant, fewer than six interventions per user or long sequences of unrecognized characters (likely images) were discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4cf9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conversations = 0\n",
    "discarded_messages = []\n",
    "conversations_clean = []\n",
    "predators_in_filtered_conversations = set()\n",
    "\n",
    "junk_pattern = re.compile(r\"[^a-zA-Z0-9áéíóúÁÉÍÓÚñÑüÜ\\s.,!?*\\'\\\"@():;<>\\/\\-]+\")\n",
    "\n",
    "# Conversations loop.\n",
    "for conversation in root_train.findall(\"conversation\"):\n",
    "    total_conversations += 1\n",
    "    authors = defaultdict(list)\n",
    "    all_texts = []\n",
    "\n",
    "    # Messages loop. Keep only those that pass the filters.\n",
    "    for message in conversation.findall(\"message\"):\n",
    "        author_el = message.find(\"author\")\n",
    "        text_el = message.find(\"text\")\n",
    "\n",
    "        if author_el is None or author_el.text is None:\n",
    "            continue\n",
    "        if text_el is None or text_el.text is None:\n",
    "            continue\n",
    "        \n",
    "        author_id = author_el.text.strip()\n",
    "        text = text_el.text.strip()\n",
    "        text = html.unescape(text) # Substitue &amp, &lt;, &gt; etc. with their characters.\n",
    "\n",
    "        # Long sequences of unrecognized characters\n",
    "        if len(text) > 20 and junk_pattern.search(text):\n",
    "            discarded_messages.append(text) \n",
    "            continue\n",
    "\n",
    "        authors[author_id].append(text)\n",
    "        all_texts.append(text)\n",
    "\n",
    "    # Conversations with only one participant\n",
    "    if len(authors) <= 1:\n",
    "        continue\n",
    "    # Conversations with fewer than six interventions per user\n",
    "    if any(len(msgs) < 6 for msgs in authors.values()):\n",
    "        continue\n",
    "\n",
    "    # If the conversation passes all filters, we keep it.\n",
    "    conversations_clean.append({\n",
    "        \"conversation_id\": conversation.get(\"id\"),\n",
    "        \"authors\": list(authors.keys()),\n",
    "        \"text\": \" \".join(all_texts),\n",
    "        \"messages_by_author\": dict(authors)\n",
    "    })\n",
    "\n",
    "    # Check if any of the authors are predators\n",
    "    for author_id in authors:\n",
    "        if author_id in predator_ids_train:\n",
    "            predators_in_filtered_conversations.add(author_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a5f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total conversations: 66927\n",
      "Valid conversations after filtering: 8210\n",
      "Predators in the original list: 142\n",
      "Predators appearing in filtered conversations: 137\n",
      "\n",
      "Some examples of discarded messages:\n",
      " 1. Cam = not such good conversation.\n",
      " 2. Well, even though I'm glad to chat w/ ya & you're quite...probably better I let you connect w/ someone a bit younger, eh?\n",
      " 3. sets mode: +oo tantek ChanServ\n",
      " 4. sets mode: +o ChanServ\n",
      " 5. for t = 1 : T-1\n",
      "    w(t) = (wi-wf)*(T-t)/T + wf ;\n",
      "    for i = 1:N\n",
      "        v_a0(i,t+1) = w(t)*v_a0(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a0(i,t+1) = p_a0(i,t) + v_a0(i,t+1);\n",
      "        v_a1(i,t+1) = w(t)*v_a1(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a1(i,t+1) = p_a1(i,t) + v_a1(i,t+1);\n",
      "        v_a2(i,t+1) = w(t)*v_a2(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a2(i,t+1) = p_a2(i,t) + v_a2(i,t+1);\n",
      "        v_a3(i,t+1) = w(t)*v_a3(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a3(i,t+1) = p_a3(i,t) + v_a3(i,t+1);\n",
      "        v_a4(i,t+1) = w(t)*v_a4(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a4(i,t+1) = p_a4(i,t) + v_a4(i,t+1);\n",
      "        v_a5(i,t+1) = w(t)*v_a5(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a5(i,t+1) = p_a5(i,t) + v_a5(i,t+1);\n",
      "        v_a6(i,t+1) = w(t)*v_a6(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a6(i,t+1) = p_a6(i,t) + v_a6(i,t+1);\n",
      "        v_a7(i,t+1) = w(t)*v_a7(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a7(i,t+1) = p_a7(i,t) + v_a7(i,t+1);\n",
      "        v_a8(i,t+1) = w(t)*v_a8(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a8(i,t+1) = p_a8(i,t) + v_a8(i,t+1);\n",
      "        v_a9(i,t+1) = w(t)*v_a9(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a9(i,t+1) = p_a9(i,t) + v_a9(i,t+1);\n",
      "        v_a10(i,t+1) = w(t)*v_a10(t) + p1*r1*(x(lbest)-x(i)) + p2*r2*(x(gbest)-x(i)) ;\n",
      "        p_a10(i,t+1) = p_a10(i,t) + v_a10(i,t+1);\n",
      "        f(i,t) = a0(i) + a1(i)*(x(i)) + a2(i)*((x(i))^2) + a3(i)*((x(i))^3) + a4(i)*((x(i))^4) + a5(i)*((x(i))^5)+ a6(i)*((x(i))^6)+ a7(i)*((x(i))^7)+ a8(i)*((x(i))^8)+ a9(i)*((x(i))^9)+ a10(i)*((x(i))^10);\n",
      "        e(i,t) = sqrt(sum((f(i,t)-y(i))^2));\n",
      "if e(i,t) == min(e(:,t))\n",
      "    gbest = i;\n",
      "end\n",
      "    end\n",
      "if f(i,t+1) > f(i,t) \n",
      "    lbest = i;\n",
      "end\n",
      "end\n",
      " 6. ok, 22813b6f9b851d152e2f2d99116de592; I see IA_WebApps(DOM3)2:00PM scheduled to start in 40 minutes\n",
      " 7. ok, 227a9a17d1db3f8e5c71a150345fb2a6; conference Team_(webapps)18:20Z scheduled with code 26631 (CONF1) for 60 minutes until 1920Z\n",
      " 8. Team_(webapps)18:20Z has now started\n",
      " 9. 17f9b78129866dff3cbd228a8d985fbf, [IPcaller] is Olli_Pettay\n",
      "10. http://www.w3.org/2008/webapps/wiki/Selector-based_Mutation_Events\n",
      "11. i got apache compield with mod _sll and most toher modules, just compiled mkd jk against it with aspx, and put all that junk in a script, have tomcat 5517 and jdk 1508 installed, got env varibales going, im almost ready\n",
      "12. could some one give me a hand on a tomcat+apache issue that I'm having ?\n",
      "13. I followed these instruction on setting it up <a href=\"http://www.meritonlinesystems.com/docs/apache_tomcat_redhat.html\">http://www.meritonlinesystems.com/docs/apache_tomcat_redhat.html</a>\n",
      "14. now I got redo my apache configs for the virtual server that is using mod_jk\n",
      "15. <a href=\"http://rafb.net/paste/results/LZxvFV98.html\">http://rafb.net/paste/results/LZxvFV98.html</a>\n",
      "16. i'm not so sure it's 100% correct\n",
      "17. <a href=\"http://jakarta/servlets-examples/servlet/HelloWorldExample\">http://jakarta/servlets-examples/servlet/HelloWorldExample</a>\n",
      "18. <a href=\"http://issues.apache.org/bugzilla/show_bug.cgi?id=34587\">http://issues.apache.org/bugzilla/show_bug.cgi?id=34587</a>\n",
      "19. tomcat is, like, at <a href=\"http://jakarta.apache.org/tomcat\">http://jakarta.apache.org/tomcat</a>\n",
      "20. SQL (Structured Query Language) database server. URL: <a href=\"http://www.mysql.com/\">http://www.mysql.com/</a>\n",
      "21. So how do I know which elements of Tomcat do I need for a webservice such as SOAP? CORE+ Deployer+ Embedded?\n",
      "22. i heard eclipse is a kind of universal tool platform - an open extensible IDE for anything and nothing in particular.  <a href=\"http://www.eclipse.org/\">http://www.eclipse.org/</a>\n",
      "23. java is probably at (Link: <a href=\"http://www.blackdown.org\">http://www.blackdown.org</a>)<a href=\"http://www.blackdown.org\">http://www.blackdown.org</a> or at (Link: <a href=\"http://www.javasoft.com\">http://www.javasoft.com</a>)<a href=\"http://www.javasoft.com\">http://www.javasoft.com</a> or at (Link: <a href=\"http://www.savaje.com\">http://www.savaje.com</a>)<a href=\"http://www.savaje.com\">http://www.savaje.com</a> or at (Link: <a href=\"http://www.pocketlinux.com\">http://www.pocketlinux.com</a>)<a href=\"http://www.pocketlinux.com\">http://www.pocketlinux.com</a> or at (Link: <a href=\"http://java.sun.com\">http://java.sun.com</a>)<a href=\"http://java.sun.com\">http://java.sun.com</a> or at <a href=\"http://www.insignia.com\">http://www.insignia.com</a>, or see the ...\n",
      "24. i was thinking c++ and java\n",
      "25. #foocamp http://is.gd/2FGOU\n",
      "26. 65325d50b2e25aca54bc871b89758c9c_: btw, did you have any opinions on chaals' reply to my comments on the progress events stuff?\n",
      "27. http://golem.ph.utexas.edu/~distler/blog/archives/001842.html :)\n",
      "28. zcorpan: Does window.FileList implement [[Call]]?\n",
      "29. zcorpan: (per ES5, host objects that implement [[Call]] must return \"function\"; host objects that don't implement call can return anything except \"undefined\", \"boolean\", \"number\", or \"string\")\n",
      "30. <a href=\"http://cgi.ebay.com.au/Cisco-ASA-5510-Adaptive-Security-Appliance_W0QQitemZ160006939689QQihZ006QQcategoryZ70813QQrdZ1QQcmdZViewItem\">http://cgi.ebay.com.au/Cisco-ASA-5510-Adaptive-Security-Appliance_W0QQitemZ160006939689QQihZ006QQcategoryZ70813QQrdZ1QQcmdZViewItem</a>\n",
      "31. if I want the value of $_GET['step'] but I want to increase it by one and keep the original value, can I use a reference?\n",
      "32. like &$_GET['step']++\n",
      "33. or should I just do $_GET['step'] + 1\n",
      "34. I want to keep the original value intact, just use the \"next\" number available (+1)\n",
      "35. What is the point in a reference? or & ?\n",
      "36. $var_one = 'asdf', $var_two = &$var_one?\n",
      "37. in \"echo '<span class=\"gensmall\">a'.$COUNT.' views.</span>';\" how does $COUNT get echoed before the span?\n",
      "38. <a href=\"http://us3.php.net/manual/en/function.rewind.php\">http://us3.php.net/manual/en/function.rewind.php</a>\n",
      "39. if (!file_exists($file)) $fptr=fopen($file,\"x+\"); else (open nomally)\n",
      "40. how hard can it bloody be to get some cyboring..?._.\n",
      "41. Bug https://bugzilla.mozilla.org/show_bug.cgi?id=548472 nor, --, ---, surkov.alexander, ASSI, Column header accessibles are not ordered according to visual order\n",
      "42. anne, that's not required.. the Process Document says \"A Working Group SHOULD work with other groups prior to a Last Call announcement to reduce the risk of surprise at Last Call.\" http://www.w3.org/2005/10/Process-20051014/tr.html#last-call  But that's about coordination during Working Draft stages, not a prerequisite for going to LC\n",
      "43. 4dd217ba7542bb00a127f682e2c3ad27: ^^\n",
      "44. I think it's a very good rule, it's just that I ignore it if the review period is >= 4 weeks\n",
      "45. perhaps I should ++ it ...\n",
      "46. planet: Firefox, Fennec, Kraken, OIN, MDN, Zaphod, Narcissus, Thunderbird, P2PU, Firefox Home, and more‚Ä¶ <http://blog.mozilla.com/about_mozilla/2010/09/21/firefox-fennec-kraken-oin-mdn-zaphod-narcissus-thunderbird-p2pu-firefox-home-and-more/>\n",
      "47. like 50% of the time it rains.\n",
      "48. i am minting URL_MISMATCH_ERR exception code 19\n",
      "49. bugmail: [Bug 11107] New: Make the expected rendering of datetime controls and number controls clearer <http://lists.w3.org/Archives/Public/public-html-bugzilla/2010Oct/2769.html>\n",
      "50. _SiegeX, good day! how are you =\n",
      "51. FUCK YOU\n",
      "....................../´¯/)\n",
      "....................,/¯../\n",
      ".................../..../\n",
      "............./´¯/'...'/´¯¯`·¸\n",
      "........../'/.../..../......./¨¯\\\n",
      "........('(...´...´.... ¯~/'...')\n",
      ".........\\.................'...../\n",
      "..........''...\\.......... _.·´\n",
      "............\\..............(\n",
      "..............\\.............\\...\n",
      "52. FUCK YOU\n",
      "....................../´¯/)\n",
      "....................,/¯../\n",
      ".................../..../\n",
      "............./´¯/'...'/´¯¯`·¸\n",
      "........../'/.../..../......./¨¯\\\n",
      "........('(...´...´.... ¯~/'...')\n",
      ".........\\.................'...../\n",
      "..........''...\\.......... _.·´\n",
      "............\\..............(\n",
      "..............\\.............\\...newfag\n",
      "53. is there a way to pop a tooltip for input type=\"image\" ???\n",
      "54. riczho, html input: <a href=\"http://www.w3.org/TR/html401/interact/forms.html#edef-INPUT\">http://www.w3.org/TR/html401/interact/forms.html#edef-INPUT</a>\n",
      "55. Try setting a title=\"\" .. though you've left now and probably wont see this message\n",
      "56. drop (select table_name from user_tables)\n",
      "57. spool drop_tables.sql\n",
      "58. select 'drop table '|| table_name ||' cascade;' from user_tables;\n",
      "59. when converting a numeric to varchar, is there a reason i should choose to_char over cast?\n",
      "60. hello, how can I convert a long type to char? I want to do something like \"select table_name || ',' || search_condition from user_constraints\" but it's not possible as search_condition is a long type, I have tried TO_CHAR but doesn't work\n",
      "61. to_char(234324, 'dddddd') ?\n",
      "62. quag|work, you can try \"select to_char(search_condition) from user_constraints\"\n",
      "63. quag|work, anyway I got an answer from google already,thx\n",
      "64. select to_char(232,'999') from dual\n",
      "65. quag|work, long mean long raw, not long integer\n",
      "66. quag|work, <a href=\"http://asktom.oracle.com/pls/ask/f?p=4950\">http://asktom.oracle.com/pls/ask/f?p=4950</a>:8:::::F4950_P8_DISPLAYID:839298816582#10650725404823\n",
      "67. schema.function_name(foo);\n",
      "68. is it right I need DBA to \"grant execute function_name\" in order to use it?\n",
      "69. grant execute on schema.function_name to user;\n",
      "70. i have install Suse enterprise 10 , and i download the 3 cpio file of oracle database 9i i start the install fine , i select all option by defaul ( is just a dabatase for test ) but the install stop in 85% of processing something for enterprise manager , i select a custom install after and uncheck that packages but now i get error with the package of apache ( apache packages of oracle install )\n",
      "71. none of gtk_file_chooser_set/select_current_file/directory work for me :(\n",
      "72. got it... gtk_file_chooser_select_uri() works...\n",
      "73. planet: Scott Knaster: Fridaygram <http://feedproxy.google.com/~r/blogspot/Dcni/~3/B0_l8-1Vd7U/fridaygram_29.html> ** Scott Knaster: Chime in on #io2011 and check out After Hours <http://feedproxy.google.com/~r/blogspot/Dcni/~3/452QKqBoJGY/chime-in-on-io2011-and-check-out-after.html>\n",
      "74. ………………….._,,-~’’’¯¯¯’’~-,,…………………………………………………………\n",
      "………………..,-‘’ ; ; ;_,,---,,_ ; ;’’-,………………………….._,,,---,,_………………\n",
      "……………….,’ ; ; ;,-‘ , , , , , ‘-, ; ;’-,,,,---~~’’’’’’~--,,,_…..,,-~’’ ; ; ; ;__;’-,……………\n",
      "……………….| ; ; ;,’ , , , _,,-~’’ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ¯’’~’-,,_ ,,-~’’ , , ‘, ;’, …………\n",
      "……………….’, ; ; ‘-, ,-~’’ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;’’-, , , , , ,’ ; |…………\n",
      "…………………’, ; ;,’’ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;’-, , ,-‘ ;,-‘…………\n",
      "………………….,’-‘ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;’’-‘ ;,,-‘…………..\n",
      "………………..,’ ; ; ; ; ; ; ; ; ; ; ; ;__ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ‘-,’……………..\n",
      "………………,-‘ ; ; ; ; ; ; ; ; ; ;,-‘’¯: : ’’-, ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; _ ; ; ; ; ;’,……………..\n",
      "……………..,’ ; ; ; ; ; ; ; ; ; ; ;| : : : : : | ; ; ; ; ; ; ; ; ; ; ; ; ,-‘’¯: ¯’’-, ; ; ;’,…………….\n",
      "…………….,’ ; ; ; ; ; ; ; ; ; ; ; ‘-,_: : _,-‘ ; ; ; ; ; ; ; ; ; ; ; ; | : : : : : | ; ; ; |……………\n",
      "……………,’ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ¯¯ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;’-,,_ : :,-‘ ; ; ; ;|……………\n",
      "…………..,-‘ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ,,-~’’ , , , , ,,,-~~-, , , , _ ; ; ;¯¯ ; ; ; ; ;|……\n",
      "…………,-‘ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;,’ , , , , , , ,( : : : : ) , , , ,’’-, ; ; ; ; ; ; ; ;|……………\n",
      "……….,-‘ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;’, , , , , , , , ,’~---~’’ , , , , , ,’ ; ; ; ; ; ; ; ;’,…………..\n",
      "…….,-‘’ ; _, ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ‘’~-,,,,--~~’’’¯’’’~-,,_ , ,_,-‘ ; ; ; ; ; ; ; ; ; ‘,………….\n",
      "….,-‘’-~’’,-‘ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; . . . . . . . ,’; ,’’¯ ; ; ; ; ; ; ; ; ; ,_ ; ‘-,………..\n",
      "……….,’ ; ;,-, ; ;, ; ; ;, ; ; ; ; ; ; ; ; ; ; ‘, ; ;’, . . . . .,’ ;,’ ; ; ; ;, ; ; ;,’-, ; ;,’ ‘’~--‘’’………\n",
      "………,’-~’ ,-‘-~’’ ‘, ,-‘ ‘, ,,- ; ; ; ; ; ; ; ; ‘, ; ; ‘~-,,,-‘’ ; ,’ ; ; ; ; ‘, ;,-‘’ ; ‘, ,-‘,……………..\n",
      "……….,-‘’ ; ; ; ; ; ‘’ ; ; ;’’ ; ; ; ; ; ; ; ; ; ; ‘’-,,_ ; ; ; _,-‘ ; ; ; ; ; ;’-‘’ ; ; ; ‘’ ; ;’-,…………..\n",
      "……..,-‘ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;¯¯’’¯ ; ; ; ; ; ; ; ; , ; ; ; ; ; ; ; ; ;’’-,……….\n",
      "……,-‘ ; ; ; ; ; ; ; ,, ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; |, ; ; ; ; ; ; ; ; ; ; ‘-,…….\n",
      "…..,’ ; ; ; ; ; ; ; ;,’ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;|..’-,_ ; ; ; , ; ; ; ; ; ‘,…..\n",
      "….,’ ; ; ; ; ; ; ; ; | ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;,’…….’’’,-~’ ; ; ; ; ; ,’…..\n",
      "…,’ ; ; ; ; ; ; ; ; ;’~-,,,,,--~~’’’’’’~-,, ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;,’…..,-~’’ ; ; ; ; ; ; ,-‘……\n",
      "…| ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ‘, ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;,’…,-‘ ; ; ; ; ; ; ; ;,-‘……..\n",
      "…’, ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ,-‘ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ,’….’, ; ; ; ; _,,-‘’…………\n",
      "….’, ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ,-‘’ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;,’…….’’~~’’¯………………\n",
      "…..’’-, ; ; ; ; ; ; ; ; ; ; ; ; ; ;_,,-‘’ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ,-‘…………………………….\n",
      "………’’~-,,_ ; ; ; ; _,,,-~’’ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;,-‘……………………………..\n",
      "………..| ; ; ;¯¯’’’’¯ ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;,,-‘……………………………….\n",
      "………..’, ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;,-‘…………………………………\n",
      "…………| ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;|…………………………………..\n",
      "…………’, ; ; ; ; ; ; ; ; ; ~-,,___ ; ; ; ; ; ; ; ; ; ; ; ; ; ;’,………………………………….\n",
      "………….’, ; ; ; ; ; ; ; ; ; ; ;,-‘….’’-, ; ; ; ; ; ; ; ; ; ; ; ; ‘,…………………………………\n",
      "………..,’ ‘- ; ; ; ; ; ; ; ; ;,-‘’……….’-, ; ; ; ; ; ; ; ; ; ; ; ‘,………………………………..\n",
      "……….,’ ; ;’ ; ; ; ; ; ; ,,-‘…………….’, ; ; ; ; ; ; ; ; ; ; ;’,……………………………….\n",
      "………,’ ; ; ; ; ; ; ; ;,-‘’…………………’’-, ; ; ; ; ; ; ; ; ; |……………………………….\n",
      "……..,’ ; ; ; ; ; ; ;,,-‘………………………’’, ; ; ; ; ; ; ; ; |………………………………\n",
      "……..| ; ; ; ; ; ; ;,’…………………………,’ ; ; ; ; ; ; ; ;,’……………………………….\n",
      "……..| ; ; ; ; ; ; ,’………………………..,-‘ ; ; ; ; ; ; ; ,’’………………………………..\n",
      "……..| ; ; ; ; ; ;,’……………………….,-‘ ; ; ; ; ; ; ; ,-‘………………………………….\n",
      "……..’,_ , ; , ;,’……………………….,’ ; ; ; ; ; ; ; ,-‘……………………………………\n",
      "………’,,’,¯,’,’’|……………………….| ; ; ; ; ; ; ; ; ‘--,,………………………………….\n",
      "………….¯…’’………………………..’-, ; ; ; ; ; ; ; ; ; ;’’~,,…………………………….\n",
      "……………………………………………’’-,, ; ; ; ; ; ; ; ; ; ;’’~-,,……………………….\n",
      "………………………………………………..’’-, ; ; ; ; ; ,,_ ; ;’-,’’-,……………………..\n",
      "…………………………………………………..’, ; ; ; ; ; ; ‘-,__,\\\\--\\\\...........................\n",
      "……………………………………………………’-, ; ; ;,,-~’’’ \\\\ , ,|, |……………………\n",
      "………………………………………………………’’~-‘’_ , , ,,’,_/--‘……………………\n",
      "…………………………………………………………….’’~-~’…….............…….\n",
      "im pedo bear from finland\n",
      "75. i has no sex im pedo bear ^^\n",
      "76. actually, it looks like I do have access to the logs in ~/logs/\n",
      "77. Krzysztof ?ªelechowski's e-mails read like poetry\n",
      "78. planet: SVG at Google and in Internet Explorer <http://feedproxy.google.com/~r/blogspot/Dcni/~3/jNXR8RPN2p0/svg-at-google-and-in-internet-explorer.html>\n",
      "79. bugmail: [Bug 10605] Typo: Replace 'the alt attribute's value may be omitted' with '@alt may be omitted' <http://lists.w3.org/Archives/Public/public-html-bugzilla/2011Jan/0677.html> ** [Bug 10618] Use \"unmapped\" rather than \"no role\" in the weak/strong ARIA tables <http://lists.w3.org/Archives/Public/public-html-bugzilla/2011Jan/0676.html> ** [Bug 10066] replace section 3.2.6 with the alternative spec text provided (ARIA) <http://lists.w3.o\n",
      "80. planet: <meta charset=utf-8> vs <meta charset=\"utf-8\"> in html 5 <http://doctype.com/meta-charsetutf8-vs-meta-charsetutf8-html-5>\n",
      "81. I've been trying to download the last 3 chunks of Elephants_Dream_HD.avi for hours now, why can't I get them are there so few people offering them and are they all so ocupied that just those 3 pieces are missing while the rest went pretty fast?\n",
      "82. there is whole thesis available, but only in czech http://relaxed.sourceforge.net/thesis_cz.html\n",
      "83. what entry do you mean? what`s hdiskpower7?\n",
      "84. doesn`t this mean that you have two paths to the lun?\n",
      "85. perhaps it`s storage fault\n",
      "86. it seems such problem couldn`t be solved via irc :)\n",
      "87. that`s the problem, anyway i`m not very familiar with EMC and its powerpaths\n",
      "88. Rydekull try this if you haven`t done it yet.. /usr/sbin/powermt display dev=all\n",
      "89. Coques: Really, I am 90% sure that the SAN is setup correctly, I cant ever be 100 percent, and we have a marginal there\n",
      "90. maybe there`s duplicated entry in ODM\n",
      "91. mrmowgli, I don't think there is any tutorials on ZBrush + Blender.. Use Sculp tool for Blender.. Almost ZBrush...\n",
      "92. we can play my MOD..its not 100% complete, but I think the quests are decent\n",
      "93. and if something yo dont like..you can let me know and maybe we can work together to fix &/or make new changes ;))\n",
      "94. bugmail: \"[Bug 12171] Define \"resolve an address\" here until there's a useful spec to reference. The lack of specification seriously hurts any attempt to improve interoperability for anything related to URLs\" (2 messages in thread) <http://lists.w3.org/Archives/Public/public-html-bugzilla/2011Feb/0960.html>\n",
      "95. Fang_: You'll have to excuse me. It has been a while since I worked on one of the LEP experiments at CERN\n",
      "96. <a href=\"http://iownmymusic.org/\">http://iownmymusic.org/</a> <a href=\"http://iownmydvds.org/\">http://iownmydvds.org/</a> .\")\n",
      "97. If you're worried about that then you _really_ want to be careful about radon gas in your cellar\n",
      "98. ok thats cool, wasnt 100% sure, i got a db9/25f -> db9/25f so im converting to f/m\n",
      "99. Offhand, I'd doubt it, since sun takes extreme pains to make each module hermetic.  ask alanc in #opensolaris for a definitive answer though\n",
      "100. rbrown_: who uses that?\n"
     ]
    }
   ],
   "source": [
    "# Number of chat conversations and sexual predators in the original data vs the filtered data. Examples of discarded messages.\n",
    "print(f\"Total conversations: {total_conversations}\")\n",
    "print(f\"Valid conversations after filtering: {len(conversations_clean)}\")\n",
    "print(f\"Predators in the original list: {len(predator_ids_train)}\")\n",
    "print(f\"Predators appearing in filtered conversations: {len(predators_in_filtered_conversations)}\")\n",
    "\n",
    "print(\"\\nSome examples of discarded messages:\")\n",
    "for i, msg in enumerate(discarded_messages[:100], 1):\n",
    "    print(f\"{i:>2}. {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ac41b",
   "metadata": {},
   "source": [
    "### 2. **Labelling data.**\n",
    "\n",
    "Label all chat conversations as suspicious if they involve at least one predator (SCI task). \n",
    "\n",
    "For these suspicious conversations, separate and label the interventions as predator or victim messages (VFP task).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10faf23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversations with predators (label=1): 856\n",
      "Conversations without predators (label=0): 7354\n"
     ]
    }
   ],
   "source": [
    "conversations = [] # List to hold conversations with labels\n",
    "interventions = [] # List to hold interventions with labels\n",
    "\n",
    "for convo in conversations_clean:\n",
    "    convo_authors = convo[\"authors\"]\n",
    "    label = 1 if any(author in predator_ids_train for author in convo_authors) else 0\n",
    "\n",
    "    # Conversations labelled\n",
    "    conversations.append({\n",
    "        \"text\": convo[\"text\"],\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # If the conversation has predators, label each intervention\n",
    "    if label == 1:\n",
    "        for author, msgs in convo[\"messages_by_author\"].items():\n",
    "            author_label = 1 if author in predator_ids_train else 0\n",
    "            full_intervention = \" \".join(msgs)\n",
    "            # Interventions labelled\n",
    "            interventions.append({\n",
    "                \"intervention\": full_intervention,\n",
    "                \"label\": author_label\n",
    "            })\n",
    "\n",
    "df_conversations = pd.DataFrame(conversations)\n",
    "df_interventions = pd.DataFrame(interventions)\n",
    "\n",
    "# Print the number of conversations with and without predators\n",
    "label_counts = df_conversations[\"label\"].value_counts()\n",
    "print(f\"Conversations with predators (label=1): {label_counts.get(1, 0)}\")\n",
    "print(f\"Conversations without predators (label=0): {label_counts.get(0, 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d08ac",
   "metadata": {},
   "source": [
    "### 3. **Suspicious Conversations Identification (SCI) stage.**\n",
    "\n",
    "#### Linear SMV, Polynomial SMV, NN(one hidden layer of 10 units)\n",
    "\n",
    "Every model was trained using a Bag-of-Words (BoW) representation, first with boolean weighting and then with a TF-IDF weighting scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66050a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCI Task:\n",
      "\n",
      "\n",
      "Model: LinearSVC BoW Boolean\n",
      "Accuracy: 0.9894 ± 0.0018\n",
      "Precision (macro): 0.9784 ± 0.0063\n",
      "Recall (macro): 0.9644 ± 0.0081\n",
      "F1-score (macro): 0.9712 ± 0.0050\n",
      "\n",
      "Model: LinearSVC TF-IDF\n",
      "Accuracy: 0.9927 ± 0.0020\n",
      "Precision (macro): 0.9870 ± 0.0053\n",
      "Recall (macro): 0.9736 ± 0.0083\n",
      "F1-score (macro): 0.9801 ± 0.0056\n",
      "\n",
      "Model: PolySVC BoW Boolean\n",
      "Accuracy: 0.9753 ± 0.0025\n",
      "Precision (macro): 0.9727 ± 0.0079\n",
      "Recall (macro): 0.8924 ± 0.0119\n",
      "F1-score (macro): 0.9276 ± 0.0079\n",
      "\n",
      "Model: PolySVC TF-IDF\n",
      "Accuracy: 0.9833 ± 0.0024\n",
      "Precision (macro): 0.9816 ± 0.0059\n",
      "Recall (macro): 0.9277 ± 0.0106\n",
      "F1-score (macro): 0.9525 ± 0.0072\n",
      "\n",
      "Model: MLP BoW Boolean\n",
      "Accuracy: 0.9892 ± 0.0062\n",
      "Precision (macro): 0.9832 ± 0.0069\n",
      "Recall (macro): 0.9580 ± 0.0294\n",
      "F1-score (macro): 0.9697 ± 0.0185\n",
      "\n",
      "Model: MLP TF-IDF\n",
      "Accuracy: 0.9945 ± 0.0022\n",
      "Precision (macro): 0.9895 ± 0.0046\n",
      "Recall (macro): 0.9810 ± 0.0095\n",
      "F1-score (macro): 0.9851 ± 0.0060\n"
     ]
    }
   ],
   "source": [
    "models_SCI = {\n",
    "    \"LinearSVC BoW Boolean\": Pipeline([\n",
    "        ('vectorizer', CountVectorizer(binary=True)),\n",
    "        ('classifier', LinearSVC(dual='auto'))\n",
    "    ]),\n",
    "    \"LinearSVC TF-IDF\": Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('classifier', LinearSVC(dual='auto'))\n",
    "    ]),\n",
    "    \"PolySVC BoW Boolean\": Pipeline([\n",
    "        ('vectorizer', CountVectorizer(binary=True)),\n",
    "        ('classifier', SVC(kernel='poly', degree=2, C=1.0))\n",
    "    ]),\n",
    "    \"PolySVC TF-IDF\": Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('classifier', SVC(kernel='poly', degree=2, C=1.0))\n",
    "    ]),\n",
    "    \"MLP BoW Boolean\": Pipeline([\n",
    "        ('vectorizer', CountVectorizer(binary=True)),\n",
    "        ('classifier', MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42))\n",
    "    ]),\n",
    "    \"MLP TF-IDF\": Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('classifier', MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "n_splits = 20\n",
    "results = {}\n",
    "\n",
    "print(f\"SCI Task:\\n\")\n",
    "for model_name, pipeline in models_SCI.items():\n",
    "    \n",
    "    accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "    \n",
    "    for seed in range(n_splits):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df_conversations[\"text\"], df_conversations[\"label\"],\n",
    "            test_size=0.2,\n",
    "            random_state=seed,\n",
    "            stratify=df_conversations[\"label\"]\n",
    "        )\n",
    "        \n",
    "        model = clone(pipeline)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        \n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(report['macro avg']['precision'])\n",
    "        recalls.append(report['macro avg']['recall'])\n",
    "        f1s.append(report['macro avg']['f1-score'])\n",
    "    \n",
    "    results[model_name] = {\n",
    "        \"accuracy_mean\": np.mean(accuracies),\n",
    "        \"accuracy_std\": np.std(accuracies),\n",
    "        \"precision_mean\": np.mean(precisions),\n",
    "        \"precision_std\": np.std(precisions),\n",
    "        \"recall_mean\": np.mean(recalls),\n",
    "        \"recall_std\": np.std(recalls),\n",
    "        \"f1_mean\": np.mean(f1s),\n",
    "        \"f1_std\": np.std(f1s)\n",
    "    }\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy_mean']:.4f} ± {metrics['accuracy_std']:.4f}\")\n",
    "    print(f\"Precision (macro): {metrics['precision_mean']:.4f} ± {metrics['precision_std']:.4f}\")\n",
    "    print(f\"Recall (macro): {metrics['recall_mean']:.4f} ± {metrics['recall_std']:.4f}\")\n",
    "    print(f\"F1-score (macro): {metrics['f1_mean']:.4f} ± {metrics['f1_std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b587e",
   "metadata": {},
   "source": [
    "The higher the polynomial degree, the worse the performance.\n",
    "\n",
    "Additionally, reducing the number of features (i.e., BoW dimensionality) negatively impacts performance. This was confirmed both through our own experiments—where limiting the number of features progressively led to lower accuracy—and in the original paper, where the same pattern was observed. Without dimensionality reduccion, 46137 features were used.\n",
    "\n",
    "In the original paper, the best model for this task was a TF-IDF Linear SVM without dimensionality reduction, achieving an accuracy of 0.9883. Their Binary NN model without dimensionality reduction reached 0.9874 accuracy. In our case, the **TF-IDF NN without dimensionality reduction**  was the best model achieving an accuracy of 0.9945."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901a774",
   "metadata": {},
   "source": [
    "### 4. **Victim From Predator disclosure (VFP) stage.**\n",
    "\n",
    "#### Linear SMV, Polynomial SMV, NN(one hidden layer of 10 units)\n",
    "\n",
    "Every model was trained using a Bag-of-Words (BoW) representation, first with boolean weighting and then with a TF-IDF weighting scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a23982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VFP Task:\n",
      "\n",
      "\n",
      "Model: LinearSVC BoW Boolean\n",
      "Accuracy: 0.9064 ± 0.0133\n",
      "Precision (macro): 0.9070 ± 0.0134\n",
      "Recall (macro): 0.9064 ± 0.0133\n",
      "F1-score (macro): 0.9064 ± 0.0133\n",
      "\n",
      "Model: LinearSVC TF-IDF\n",
      "Accuracy: 0.9394 ± 0.0114\n",
      "Precision (macro): 0.9397 ± 0.0114\n",
      "Recall (macro): 0.9394 ± 0.0114\n",
      "F1-score (macro): 0.9393 ± 0.0114\n",
      "\n",
      "Model: PolySVC BoW Boolean\n",
      "Accuracy: 0.8491 ± 0.0166\n",
      "Precision (macro): 0.8661 ± 0.0147\n",
      "Recall (macro): 0.8492 ± 0.0166\n",
      "F1-score (macro): 0.8473 ± 0.0172\n",
      "\n",
      "Model: PolySVC TF-IDF\n",
      "Accuracy: 0.9369 ± 0.0111\n",
      "Precision (macro): 0.9372 ± 0.0110\n",
      "Recall (macro): 0.9369 ± 0.0111\n",
      "F1-score (macro): 0.9369 ± 0.0111\n",
      "\n",
      "Model: MLP BoW Boolean\n",
      "Accuracy: 0.9388 ± 0.0095\n",
      "Precision (macro): 0.9392 ± 0.0096\n",
      "Recall (macro): 0.9388 ± 0.0095\n",
      "F1-score (macro): 0.9388 ± 0.0095\n",
      "\n",
      "Model: MLP TF-IDF\n",
      "Accuracy: 0.9446 ± 0.0089\n",
      "Precision (macro): 0.9451 ± 0.0089\n",
      "Recall (macro): 0.9446 ± 0.0089\n",
      "F1-score (macro): 0.9446 ± 0.0089\n"
     ]
    }
   ],
   "source": [
    "models_VPF = {\n",
    "    \"LinearSVC BoW Boolean\": Pipeline([\n",
    "        ('vectorizer', CountVectorizer(binary=True)),\n",
    "        ('classifier', LinearSVC(dual='auto'))\n",
    "    ]),\n",
    "    \"LinearSVC TF-IDF\": Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(max_features=5000)),\n",
    "        ('classifier', LinearSVC(dual='auto'))\n",
    "    ]),\n",
    "    \"PolySVC BoW Boolean\": Pipeline([\n",
    "        ('vectorizer', CountVectorizer(binary=True)),\n",
    "        ('classifier', SVC(kernel='poly', degree=2, C=1.0))\n",
    "    ]),\n",
    "    \"PolySVC TF-IDF\": Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(max_features=5000)),\n",
    "        ('classifier', SVC(kernel='poly', degree=2, C=1.0))\n",
    "    ]),\n",
    "    \"MLP BoW Boolean\": Pipeline([\n",
    "        ('vectorizer', CountVectorizer(binary=True)),\n",
    "        ('classifier', MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42))\n",
    "    ]),\n",
    "    \"MLP TF-IDF\": Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(max_features=5000)),\n",
    "        ('classifier', MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "n_splits = 20\n",
    "results = {}\n",
    "\n",
    "print(f\"VFP Task:\\n\")\n",
    "for model_name, pipeline in models_VPF.items():\n",
    "    \n",
    "    accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "    \n",
    "    for seed in range(n_splits):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df_interventions[\"intervention\"], df_interventions[\"label\"],\n",
    "            test_size=0.2,\n",
    "            random_state=seed,\n",
    "            stratify=df_interventions[\"label\"]\n",
    "        )\n",
    "        \n",
    "        model = clone(pipeline)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        \n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(report['macro avg']['precision'])\n",
    "        recalls.append(report['macro avg']['recall'])\n",
    "        f1s.append(report['macro avg']['f1-score'])\n",
    "    \n",
    "    results[model_name] = {\n",
    "        \"accuracy_mean\": np.mean(accuracies),\n",
    "        \"accuracy_std\": np.std(accuracies),\n",
    "        \"precision_mean\": np.mean(precisions),\n",
    "        \"precision_std\": np.std(precisions),\n",
    "        \"recall_mean\": np.mean(recalls),\n",
    "        \"recall_std\": np.std(recalls),\n",
    "        \"f1_mean\": np.mean(f1s),\n",
    "        \"f1_std\": np.std(f1s)\n",
    "    }\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy_mean']:.4f} ± {metrics['accuracy_std']:.4f}\")\n",
    "    print(f\"Precision (macro): {metrics['precision_mean']:.4f} ± {metrics['precision_std']:.4f}\")\n",
    "    print(f\"Recall (macro): {metrics['recall_mean']:.4f} ± {metrics['recall_std']:.4f}\")\n",
    "    print(f\"F1-score (macro): {metrics['f1_mean']:.4f} ± {metrics['f1_std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dfce2b",
   "metadata": {},
   "source": [
    "For the **VFP task**, the **best-performing model** was a **NN** using a Bag-of-Words (BoW) representation **with TF-IDF weighting and dimensionality reduction** — from 9610 **features** down to **5000**. As previously observed, increasing the polynomial degree in SVM leads to poorer performance.\n",
    "\n",
    "In the original paper, the best model for this task was a Boolean NN without dimensionality reduction, achieving an accuracy of 0.9407. In our case, the best model achieved an accuracy of 0.9446."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b9e02",
   "metadata": {},
   "source": [
    "### 5. **Evaluate test data using best performance model:** SCI(NN-TF_IDF)& VFP(NN-TF_IDF-DimensionalityReduction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "730b69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pan12-sexual-predator-identification-groundtruth-problem1.txt\") as f:\n",
    "    predator_ids_test = set(f.read().splitlines())\n",
    "\n",
    "tree_test = ET.parse(\"pan12-sexual-predator-identification-test-corpus-2012-05-17.xml\")\n",
    "root_test = tree_test.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67c1f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredatorDetectionPipeline:\n",
    "    def __init__(self, conversation_model, author_model):\n",
    "        self.conversation_model = conversation_model\n",
    "        self.author_model = author_model\n",
    "        self.junk_pattern = re.compile(r\"[^a-zA-Z0-9áéíóúÁÉÍÓÚñÑüÜ\\s.,!?*\\'\\\"@():;<>\\/\\-]+\")\n",
    "\n",
    "    def _filter(self, root):\n",
    "        filtered_conversations = []\n",
    "        for conversation in root.findall(\"conversation\"):\n",
    "            authors = defaultdict(list)\n",
    "            all_texts = []\n",
    "\n",
    "            for message in conversation.findall(\"message\"):\n",
    "                author_el = message.find(\"author\")\n",
    "                text_el = message.find(\"text\")\n",
    "\n",
    "                if author_el is None or author_el.text is None:\n",
    "                    continue\n",
    "                if text_el is None or text_el.text is None:\n",
    "                    continue\n",
    "\n",
    "                author_id = author_el.text.strip()\n",
    "                text = html.unescape(text_el.text.strip())\n",
    "\n",
    "                if len(text) > 20 and self.junk_pattern.search(text):\n",
    "                    continue\n",
    "\n",
    "                authors[author_id].append(text)\n",
    "                all_texts.append(text)\n",
    "\n",
    "            if len(authors) <= 1:\n",
    "                continue\n",
    "            if any(len(msgs) < 6 for msgs in authors.values()):\n",
    "                continue\n",
    "\n",
    "            filtered_conversations.append({\n",
    "                \"conversation_id\": conversation.get(\"id\"),\n",
    "                \"authors\": list(authors.keys()),\n",
    "                \"text\": \" \".join(all_texts),\n",
    "                \"messages_by_author\": dict(authors),\n",
    "                \"xml\": conversation\n",
    "            })\n",
    "\n",
    "        return filtered_conversations\n",
    "\n",
    "    def _true_labels(self, conversations, predator_ids):\n",
    "        true_labels = []\n",
    "        for convo in conversations:\n",
    "            convo_authors = convo[\"authors\"]\n",
    "            predator_in_convo = [author for author in convo_authors if author in predator_ids]\n",
    "\n",
    "            if predator_in_convo:\n",
    "                true_labels.append({\n",
    "                    \"conversation_id\": convo[\"conversation_id\"],\n",
    "                    \"suspicious\": True,\n",
    "                    \"predator\": predator_in_convo[0]\n",
    "                })\n",
    "            else:\n",
    "                true_labels.append({\n",
    "                    \"conversation_id\": convo[\"conversation_id\"],\n",
    "                    \"suspicious\": False,\n",
    "                    \"predator\": None\n",
    "                })\n",
    "\n",
    "        return true_labels\n",
    "\n",
    "    def _predict(self, conversation_data):\n",
    "        text = conversation_data[\"text\"]\n",
    "        is_suspicious = self.conversation_model.predict([text])[0]\n",
    "        if not is_suspicious:\n",
    "            return {\n",
    "                \"conversation_id\": conversation_data[\"conversation_id\"],\n",
    "                \"suspicious\": False,\n",
    "                \"predator\": None\n",
    "            }\n",
    "\n",
    "        author_probs = []\n",
    "        for author_id, msgs in conversation_data[\"messages_by_author\"].items():\n",
    "            joined_msgs = \" \".join(msgs)\n",
    "            score = self.author_model.predict_proba([joined_msgs])[0][1]\n",
    "            author_probs.append((author_id, score))\n",
    "\n",
    "        predator_id = max(author_probs, key=lambda x: x[1])[0]\n",
    "\n",
    "        return {\n",
    "            \"conversation_id\": conversation_data[\"conversation_id\"],\n",
    "            \"suspicious\": True,\n",
    "            \"predator\": predator_id\n",
    "        }\n",
    "\n",
    "    def run_pipeline(self, root, predator_ids):\n",
    "        # 1. Filter conversations\n",
    "        conversations = self._filter(root)\n",
    "\n",
    "        # 2. Generate true labels\n",
    "        true_labels = self._true_labels(conversations, predator_ids)\n",
    "\n",
    "        # 3. Predict with the models\n",
    "        predictions = []\n",
    "        for convo in conversations:\n",
    "            resultado = self._predict(convo)\n",
    "            predictions.append(resultado)\n",
    "\n",
    "        # 4. Compare true labels vs. predictions\n",
    "        y_true = [et[\"suspicious\"] for et in true_labels]\n",
    "        y_pred = [pr[\"suspicious\"] for pr in predictions]\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        print(\"Combined model: SCI(NN-TF_IDF)& VFP(NN-TF_IDF-DimensionalityReduction)\")\n",
    "        print(f\"Accuracy : {acc:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall   : {recall:.4f}\")\n",
    "        print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "        return {\n",
    "            \"conversation_metrics\": {\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1\n",
    "            },\n",
    "            \"predictions\": predictions,\n",
    "            \"true_labels\": true_labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7a7e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best models using all the training data\n",
    "conversation_model = models_SCI[\"MLP TF-IDF\"].fit(df_conversations[\"text\"], df_conversations[\"label\"])\n",
    "author_model = models_VPF[\"MLP TF-IDF\"].fit(df_interventions[\"intervention\"], df_interventions[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edff6ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined model: SCI(NN-TF_IDF)& VFP(NN-TF_IDF-DimensionalityReduction)\n",
      "Accuracy : 0.9930\n",
      "Precision: 0.9972\n",
      "Recall   : 0.9180\n",
      "F1 Score : 0.9560\n"
     ]
    }
   ],
   "source": [
    "pipeline = PredatorDetectionPipeline(conversation_model, author_model)\n",
    "results = pipeline.run_pipeline(root_test, predator_ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60045c6f",
   "metadata": {},
   "source": [
    "The original paper obtained precision of 0.9804, recall of 0.7874 and F1 score of 0.8734."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
