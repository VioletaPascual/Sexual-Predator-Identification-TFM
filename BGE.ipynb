{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d613815",
   "metadata": {},
   "source": [
    "# **Sexual Predator Identification BGE Embedder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28e5f0",
   "metadata": {},
   "source": [
    "### 0. 1. Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740516b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afd5bb",
   "metadata": {},
   "source": [
    "### 0.2. Load data. Predator's ID and Chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f65600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pan12-sexual-predator-identification-training-corpus-predators-2012-05-01.txt\") as f:\n",
    "    predator_ids_train = set(f.read().splitlines())\n",
    "\n",
    "tree_train = ET.parse(\"pan12-sexual-predator-identification-training-corpus-2012-05-01.xml\")\n",
    "root_train = tree_train.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c7611",
   "metadata": {},
   "source": [
    "### 1. **Filtering stage.**\n",
    "\n",
    "Conversations with only one participant, fewer than six interventions per user or long sequences of unrecognized characters (likely images) were discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4cf9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_conversations = 0\n",
    "#discarded_messages = []\n",
    "conversations_clean = []\n",
    "#predators_in_filtered_conversations = set()\n",
    "\n",
    "junk_pattern = re.compile(r\"[^a-zA-Z0-9áéíóúÁÉÍÓÚñÑüÜ\\s.,!?*\\'\\\"@():;<>\\/\\-]+\")\n",
    "\n",
    "# Conversations loop.\n",
    "for conversation in root_train.findall(\"conversation\"):\n",
    "    #total_conversations += 1\n",
    "    authors = defaultdict(list)\n",
    "    all_texts = []\n",
    "\n",
    "    # Messages loop. Keep only those that pass the filters.\n",
    "    for message in conversation.findall(\"message\"):\n",
    "        author_el = message.find(\"author\")\n",
    "        text_el = message.find(\"text\")\n",
    "\n",
    "        if author_el is None or author_el.text is None:\n",
    "            continue\n",
    "        if text_el is None or text_el.text is None:\n",
    "            continue\n",
    "        \n",
    "        author_id = author_el.text.strip()\n",
    "        text = text_el.text.strip()\n",
    "        text = html.unescape(text) # Substitue &amp, &lt;, &gt; etc. with their characters.\n",
    "\n",
    "        # Long sequences of unrecognized characters\n",
    "        if len(text) > 20 and junk_pattern.search(text):\n",
    "            #discarded_messages.append(text) \n",
    "            continue\n",
    "\n",
    "        authors[author_id].append(text)\n",
    "        all_texts.append(text)\n",
    "\n",
    "    # Conversations with only one participant\n",
    "    if len(authors) <= 1:\n",
    "        continue\n",
    "    # Conversations with fewer than six interventions per user\n",
    "    if any(len(msgs) < 6 for msgs in authors.values()):\n",
    "        continue\n",
    "\n",
    "    # If the conversation passes all filters, we keep it.\n",
    "    conversations_clean.append({\n",
    "        \"conversation_id\": conversation.get(\"id\"),\n",
    "        \"authors\": list(authors.keys()),\n",
    "        \"text\": \" \".join(all_texts),\n",
    "        \"messages_by_author\": dict(authors)\n",
    "    })\n",
    "\n",
    "    # Check if any of the authors are predators\n",
    "    #for author_id in authors:\n",
    "        #if author_id in predator_ids_train:\n",
    "            #predators_in_filtered_conversations.add(author_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ac41b",
   "metadata": {},
   "source": [
    "### 2. **Labelling data.**\n",
    "\n",
    "Label all chat conversations as suspicious if they involve at least one predator (SCI task). \n",
    "\n",
    "For these suspicious conversations, separate and label the interventions as predator or victim messages (VFP task).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10faf23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = [] # List to hold conversations with labels\n",
    "interventions = [] # List to hold interventions with labels\n",
    "\n",
    "for convo in conversations_clean:\n",
    "    convo_authors = convo[\"authors\"]\n",
    "    label = 1 if any(author in predator_ids_train for author in convo_authors) else 0\n",
    "\n",
    "    # Conversations labelled\n",
    "    conversations.append({\n",
    "        \"text\": convo[\"text\"],\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # If the conversation has predators, label each intervention\n",
    "    if label == 1:\n",
    "        for author, msgs in convo[\"messages_by_author\"].items():\n",
    "            author_label = 1 if author in predator_ids_train else 0\n",
    "            full_intervention = \" \".join(msgs)\n",
    "            # Interventions labelled\n",
    "            interventions.append({\n",
    "                \"intervention\": full_intervention,\n",
    "                \"label\": author_label\n",
    "            })\n",
    "\n",
    "df_conversations = pd.DataFrame(conversations)\n",
    "df_interventions = pd.DataFrame(interventions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d08ac",
   "metadata": {},
   "source": [
    "### 3. **Suspicious Conversations Identification (SCI) stage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4959e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"./bge-base-en\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa32fac0-8df4-4a15-99d8-12cb15879e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIC: BGE-large + MLPClassifier:\n",
      "\n",
      "Accuracy: 0.9904 ± 0.0022\n",
      "Precision (macro): 0.9778 ± 0.0092\n",
      "Recall (macro): 0.9706 ± 0.0079\n",
      "F1-score (macro): 0.9740 ± 0.0060\n"
     ]
    }
   ],
   "source": [
    "n_splits= 10\n",
    "accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "\n",
    "for seed in range(n_splits):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_conversations[\"text\"], df_conversations[\"label\"],\n",
    "        test_size=0.2, random_state=seed, stratify=df_conversations[\"label\"]\n",
    "    )\n",
    "\n",
    "    X_train_embed = embedder.encode(X_train.tolist())\n",
    "    X_test_embed = embedder.encode(X_test.tolist())\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42)\n",
    "    clf.fit(X_train_embed, y_train)\n",
    "    y_pred = clf.predict(X_test_embed)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    precisions.append(report[\"macro avg\"][\"precision\"])\n",
    "    recalls.append(report[\"macro avg\"][\"recall\"])\n",
    "    f1s.append(report[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy_mean\": np.mean(accuracies),\n",
    "    \"accuracy_std\": np.std(accuracies),\n",
    "    \"precision_mean\": np.mean(precisions),\n",
    "    \"precision_std\": np.std(precisions),\n",
    "    \"recall_mean\": np.mean(recalls),\n",
    "    \"recall_std\": np.std(recalls),\n",
    "    \"f1_mean\": np.mean(f1s),\n",
    "    \"f1_std\": np.std(f1s),\n",
    "}\n",
    "\n",
    "print(\"SIC: BGE-large + MLPClassifier:\\n\")\n",
    "print(f\"Accuracy: {metrics['accuracy_mean']:.4f} ± {metrics['accuracy_std']:.4f}\")\n",
    "print(f\"Precision (macro): {metrics['precision_mean']:.4f} ± {metrics['precision_std']:.4f}\")\n",
    "print(f\"Recall (macro): {metrics['recall_mean']:.4f} ± {metrics['recall_std']:.4f}\")\n",
    "print(f\"F1-score (macro): {metrics['f1_mean']:.4f} ± {metrics['f1_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e8f7a",
   "metadata": {},
   "source": [
    "Using TF_IDF we obtained:\n",
    "\n",
    "Accuracy: 0.9945 ± 0.0022\n",
    "\n",
    "Precision (macro): 0.9895 ± 0.0046\n",
    "\n",
    "Recall (macro): 0.9810 ± 0.0095\n",
    "\n",
    "F1-score (macro): 0.9851 ± 0.0060"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901a774",
   "metadata": {},
   "source": [
    "### 4. **Victim From Predator disclosure (VFP) stage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03402fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VFP: BGE-large + MLPClassifier:\n",
      "\n",
      "Accuracy: 0.8790 ± 0.0118\n",
      "Precision (macro): 0.8798 ± 0.0119\n",
      "Recall (macro): 0.8790 ± 0.0118\n",
      "F1-score (macro): 0.8790 ± 0.0118\n"
     ]
    }
   ],
   "source": [
    "n_splits= 10\n",
    "accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "\n",
    "for seed in range(n_splits):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_interventions[\"intervention\"], df_interventions[\"label\"],\n",
    "        test_size=0.2, random_state=seed, stratify=df_interventions[\"label\"]\n",
    "    )\n",
    "\n",
    "    X_train_embed = embedder.encode(X_train.tolist())\n",
    "    X_test_embed = embedder.encode(X_test.tolist())\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)\n",
    "    clf.fit(X_train_embed, y_train)\n",
    "    y_pred = clf.predict(X_test_embed)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    precisions.append(report[\"macro avg\"][\"precision\"])\n",
    "    recalls.append(report[\"macro avg\"][\"recall\"])\n",
    "    f1s.append(report[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy_mean\": np.mean(accuracies),\n",
    "    \"accuracy_std\": np.std(accuracies),\n",
    "    \"precision_mean\": np.mean(precisions),\n",
    "    \"precision_std\": np.std(precisions),\n",
    "    \"recall_mean\": np.mean(recalls),\n",
    "    \"recall_std\": np.std(recalls),\n",
    "    \"f1_mean\": np.mean(f1s),\n",
    "    \"f1_std\": np.std(f1s),\n",
    "}\n",
    "\n",
    "print(\"VFP: BGE-large + MLPClassifier:\\n\")\n",
    "print(f\"Accuracy: {metrics['accuracy_mean']:.4f} ± {metrics['accuracy_std']:.4f}\")\n",
    "print(f\"Precision (macro): {metrics['precision_mean']:.4f} ± {metrics['precision_std']:.4f}\")\n",
    "print(f\"Recall (macro): {metrics['recall_mean']:.4f} ± {metrics['recall_std']:.4f}\")\n",
    "print(f\"F1-score (macro): {metrics['f1_mean']:.4f} ± {metrics['f1_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc4e87",
   "metadata": {},
   "source": [
    "Using TF_IDF we obtained:\n",
    "\n",
    "Accuracy: 0.9446 ± 0.0089\n",
    "\n",
    "Precision (macro): 0.9451 ± 0.0089\n",
    "\n",
    "Recall (macro): 0.9446 ± 0.0089\n",
    "\n",
    "F1-score (macro): 0.9446 ± 0.0089"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b9e02",
   "metadata": {},
   "source": [
    "### 5. **Evaluate test data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "730b69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pan12-sexual-predator-identification-groundtruth-problem1.txt\") as f:\n",
    "    predator_ids_test = set(f.read().splitlines())\n",
    "\n",
    "tree_test = ET.parse(\"pan12-sexual-predator-identification-test-corpus-2012-05-17.xml\")\n",
    "root_test = tree_test.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67c1f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredatorDetectionPipeline:\n",
    "    def __init__(self, conversation_embedder, conversation_classifier, author_embedder=None, author_classifier=None):\n",
    "        self.conversation_embedder = conversation_embedder\n",
    "        self.conversation_classifier = conversation_classifier\n",
    "        self.author_embedder = author_embedder if author_embedder else conversation_embedder\n",
    "        self.author_classifier = author_classifier if author_classifier else conversation_classifier\n",
    "        self.junk_pattern = re.compile(r\"[^a-zA-Z0-9áéíóúÁÉÍÓÚñÑüÜ\\s.,!?*\\'\\\"@():;<>\\/\\-]+\")\n",
    "\n",
    "    def _filter(self, root):\n",
    "        filtered_conversations = []\n",
    "        for conversation in root.findall(\"conversation\"):\n",
    "            authors = defaultdict(list)\n",
    "            all_texts = []\n",
    "\n",
    "            for message in conversation.findall(\"message\"):\n",
    "                author_el = message.find(\"author\")\n",
    "                text_el = message.find(\"text\")\n",
    "\n",
    "                if author_el is None or author_el.text is None:\n",
    "                    continue\n",
    "                if text_el is None or text_el.text is None:\n",
    "                    continue\n",
    "\n",
    "                author_id = author_el.text.strip()\n",
    "                text = html.unescape(text_el.text.strip())\n",
    "\n",
    "                if len(text) > 20 and self.junk_pattern.search(text):\n",
    "                    continue\n",
    "\n",
    "                authors[author_id].append(text)\n",
    "                all_texts.append(text)\n",
    "\n",
    "            if len(authors) <= 1:\n",
    "                continue\n",
    "            if any(len(msgs) < 6 for msgs in authors.values()):\n",
    "                continue\n",
    "\n",
    "            filtered_conversations.append({\n",
    "                \"conversation_id\": conversation.get(\"id\"),\n",
    "                \"authors\": list(authors.keys()),\n",
    "                \"text\": \" \".join(all_texts),\n",
    "                \"messages_by_author\": dict(authors),\n",
    "                \"xml\": conversation\n",
    "            })\n",
    "\n",
    "        return filtered_conversations\n",
    "\n",
    "    def _true_labels(self, conversations, predator_ids):\n",
    "        true_labels = []\n",
    "        for convo in conversations:\n",
    "            convo_authors = convo[\"authors\"]\n",
    "            predator_in_convo = [author for author in convo_authors if author in predator_ids]\n",
    "\n",
    "            if predator_in_convo:\n",
    "                true_labels.append({\n",
    "                    \"conversation_id\": convo[\"conversation_id\"],\n",
    "                    \"suspicious\": True,\n",
    "                    \"predator\": predator_in_convo[0]\n",
    "                })\n",
    "            else:\n",
    "                true_labels.append({\n",
    "                    \"conversation_id\": convo[\"conversation_id\"],\n",
    "                    \"suspicious\": False,\n",
    "                    \"predator\": None\n",
    "                })\n",
    "\n",
    "        return true_labels\n",
    "\n",
    "    def _predict(self, conversation_data):\n",
    "        text = conversation_data[\"text\"]\n",
    "        embedded_text = self.conversation_embedder.encode([text])\n",
    "        is_suspicious = self.conversation_classifier.predict(embedded_text)[0]\n",
    "\n",
    "        if not is_suspicious:\n",
    "            return {\n",
    "                \"conversation_id\": conversation_data[\"conversation_id\"],\n",
    "                \"suspicious\": False,\n",
    "                \"predator\": None\n",
    "            }\n",
    "\n",
    "        author_probs = []\n",
    "        for author_id, msgs in conversation_data[\"messages_by_author\"].items():\n",
    "            joined_msgs = \" \".join(msgs)\n",
    "            author_embedding = self.author_embedder.encode([joined_msgs])\n",
    "            score = self.author_classifier.predict_proba(author_embedding)[0][1]\n",
    "            author_probs.append((author_id, score))\n",
    "\n",
    "        predator_id = max(author_probs, key=lambda x: x[1])[0]\n",
    "\n",
    "        return {\n",
    "            \"conversation_id\": conversation_data[\"conversation_id\"],\n",
    "            \"suspicious\": True,\n",
    "            \"predator\": predator_id\n",
    "        }\n",
    "\n",
    "    def run_pipeline(self, root, predator_ids):\n",
    "        # 1. Filter conversations\n",
    "        conversations = self._filter(root)\n",
    "\n",
    "        # 2. Generate true labels\n",
    "        true_labels = self._true_labels(conversations, predator_ids)\n",
    "\n",
    "        # 3. Predict with the models\n",
    "        predictions = []\n",
    "        for convo in conversations:\n",
    "            resultado = self._predict(convo)\n",
    "            predictions.append(resultado)\n",
    "\n",
    "        # 4. Compare true labels vs. predictions\n",
    "        y_true = [et[\"suspicious\"] for et in true_labels]\n",
    "        y_pred = [pr[\"suspicious\"] for pr in predictions]\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        print(\"Model evaluation:\")\n",
    "        print(f\"Accuracy : {acc:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall   : {recall:.4f}\")\n",
    "        print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "        return {\n",
    "            \"conversation_metrics\": {\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1\n",
    "            },\n",
    "            \"predictions\": predictions,\n",
    "            \"true_labels\": true_labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "051344cd-63a3-4ffe-a11a-2453e428b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation:\n",
      "Accuracy : 0.9935\n",
      "Precision: 0.9757\n",
      "Recall   : 0.9447\n",
      "F1 Score : 0.9599\n"
     ]
    }
   ],
   "source": [
    "# Train conversation model\n",
    "X_train_embed = embedder.encode(df_conversations[\"text\"].tolist())\n",
    "clf_convo = MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42)\n",
    "clf_convo.fit(X_train_embed, df_conversations[\"label\"])\n",
    "\n",
    "# Train author model\n",
    "X_author_embed = embedder.encode(df_interventions[\"intervention\"].tolist())\n",
    "clf_author = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)\n",
    "clf_author.fit(X_author_embed, df_interventions[\"label\"])\n",
    "\n",
    "# Create the pipeline and run it in the test set\n",
    "pipeline = PredatorDetectionPipeline(\n",
    "    conversation_embedder=embedder,\n",
    "    conversation_classifier=clf_convo,\n",
    "    author_embedder=embedder,\n",
    "    author_classifier=clf_author\n",
    ")\n",
    "\n",
    "results = pipeline.run_pipeline(root_test, predator_ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60045c6f",
   "metadata": {},
   "source": [
    "Using TF_IDF we obtained:\n",
    "\n",
    "\n",
    "Accuracy : 0.9930\n",
    "\n",
    "Precision: 0.9972\n",
    "\n",
    "Recall   : 0.9180\n",
    "\n",
    "F1 Score : 0.9560\n",
    "\n",
    "\n",
    "The original paper obtained precision of 0.9804, recall of 0.7874 and F1 score of 0.8734. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy Kernel",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
